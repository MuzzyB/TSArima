---
title: "Time Series Analysis Template"
author: "Muuzaani Nkhoma"
date: "May 28, 2018"
output: html_document
---

##### Load Packages run once before running the program or as needed
```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(zoo)
library(car)
library(forecast)
library(fpp)
library(tidyverse)

```


####1. DATA COLLECTION AND PREPARATION

##### Read, Explore, and Prepare the data

```{r 1a}

cheese <- read.csv("Data Time Series/Table B.4  US Production of Blue and Gorgonzola Cheeses.csv", header = TRUE)

dim(cheese)
ncol(cheese)
nrow(cheese)
names(cheese)

```

##### View first observations

```{r}

head(cheese)

```

##### View last observations

```{r}

tail(cheese)

```

##### Summary Statistics

```{r}

summary(cheese)

```

#####Create Times Series Data

```{r 1b}

#time series of US Cheese Production 
cheese.ts <- ts(cheese$Production, start = 1950, end = 1997) #If data has yearly intervals
#For monthly intervals
#elec.ts <- ts(elec$Avg.Price, start = c(2001, 1), end = c(2010, 12), frequency = 12)
#champ.ts <- ts(champ$Sales, start = c(1962, 1), frequency = 12)

```

####2. DATA ANALYSIS
#### a. Graphical Analysis
#####Time Series Plots, Scatterplots, and Histograms for Visual Inspection
#####Check for identifiable Pattern, trends, stationarity, and seasonality

#####Plot Time Series Data

```{r 2.a}

plot.ts(cheese.ts, type = "o", pch = 16, cex = .5, xlab = 'Year', 
        ylab = 'Production', main = "Us Production of Blue and Gorgonzola Cheese")

```


#### b.Numerical Analysis
#####Plot ACF and PACF

```{r 2.b}

par(mfrow=c(1,2),oma=c(0,0,2,0))
acf(cheese.ts,lag.max = 25,type = "correlation",  main = "ACF for Model #")
acf(cheese.ts, lag.max = 25, type = "partial", main = "PACF for Model #")

```

#####c.1 Transformations
#####Check for volatility 

```{r 2c1}

cheese.model<-lm(cheese$Production~cheese$Year) 
plot(cheese,type="l",xlab='Year',ylab='Production, 10000lb') 
points(cheese,pch=16,cex=.5) 
lines(cheese$Year, cheese.model$fit,col="red",lty=2) 
legend(1990,12000,c("Actual","Fits"), pch=c(16,NA),lwd=c(.5,.5),lty=c(1,2),cex=.55,col=c("black","red"))

```

####c.2.Residual Plots
#####Check for Nonconstant Variance(Heteroscedasticity)

```{r c.2}

par(mfrow=c(2,2),oma=c(0,0,0,0)) 
qqnorm(cheese.model$res,datax=TRUE,pch=16,xlab='Residual',main='Normal Q-Q Plot') 
qqline(cheese.model$res,datax=TRUE) 
plot(cheese.model$fit,cheese.model$res,pch=16, xlab='Fitted Value', ylab='Residual', main = 'Residuals vs Fitted Plot') 
abline(h=0) 
hist(cheese.model$res,col="gray",xlab='Residual',main='Residuals Histogram') 
plot(cheese.model$res,type="l",xlab='Observation Order', ylab='Residual', main = 'Residuals Plot') 
points(cheese.model$res,pch=16,cex=.5) 
abline(h=0)

```

####c.3.Box-Cox Transformation

```{r c.3}

lambda = BoxCox.lambda(cheese.ts)
cheese.model2 = BoxCox(cheese.ts, lambda=lambda)
plot.ts(cheese.model2, type = "o", pch = 16, cex = .5, xlab = 'Year', 
        ylab = 'Production', main = "Transformed Model of\n Us Production of Blue and Gorgonzola Cheese")
tsdisplay(cheese.model2)

```

####d.1 Trend and Seasonal Adjustments (Differencing)
#####Detect Trend and Seasonality
#####d.1.a Decomposition

#####d.1. Decomposition Plot

```{r d.1.b, eval=FALSE}

cheese.ts %>% decompose %>% autoplot

```

#####d.2 Detect Seasonality
#####d.2.a Seasonal Plot

```{r d.2.a, eval=FALSE}

seasonplot(cheese.ts)

```

#####d.2.b Monthly Plot

```{r d.2.b, eval=FALSE}

monthplot(cheese.ts)

```

#####d.2.c Differencing for Trend and Seasonality

```{r d.2.c, eval=FALSE}

monthplot(cheese.ts)

```

#####d.2.d Fit Seasonal ARIMA Model

```{r d.2.d, eval=FALSE}

monthplot(cheese.ts)

```

#####e. Trend and Seasonal Adjustments (Differencing)
#####Detect Trend and Nonstationarity

#####e.1 Non-Stationarity
#####Detect Non-Stationary Data

The stationarity of the data can be known by applying Unit Root Tests - Augmented Dickey-Fuller test (ADF), Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test.
Augmented Dickey-Fuller test (ADF)
The null-hypothesis for an ADF test is that the data are non-stationary. So p-value greater than 0.05 indicates non-stationarity, and  p-values less than 0.05 suggest stationarity.
KPSS Test
In this case, the null-hypothesis is that the data are stationary. In this case, p-value less than 0.05 indicates non-stationary series and p-value greater than 0.05 indicates stationary series.

```{r e.1}

# Unit Ratio Tests 
library(tseries)
adf = adf.test(cheese.ts) 
kpss = kpss.test(cheese.ts) 
adf
kpss

```

#####e.2 How to treat Non-Stationary Data

```{r e.2}

# Number of Differences Required to make data stationary 
ndiffs(cheese.ts)
cheese.ts1 = diff(cheese.ts, differences = 2)
plot.ts(cheese.ts1)

```



####3.MODEL FITTING AND SELECTION-ARIMA
###$3.a. Graphical Analysis

#####Plot Time Series Data of the stationary and differenced time series

```{r 3.a}

plot.ts(cheese.ts, type = "o", pch = 16, cex = .5, xlab = 'Year', 
        ylab = 'Production', main = "Us Production of Blue and Gorgonzola Cheese")
#tsdisplay(cheese.ts)
```

#####3.b Model Identification
#####Choose candidate ARIMA models using  ACF and PACF functions

#####b.1. ACF
```{r 3.b.1}

par(mfrow=c(1,1),oma=c(0,0,0,0))
acf(cheese.ts,lag.max = 25,type = "correlation",  main = "ACF for Model #")

```


#####b.2. PACF
```{r 3.b.2}

acf(cheese.ts, lag.max = 25, type = "partial", main = "PACF for Model #")

```

#### Choose candidate ARIMA models using autoarima(), and minimum AIC/BIC Criteria


#####b.3. Automatic Selection Algorithm - Fast
```{r 3.b.3}

auto.arima(cheese.ts, trace= TRUE, ic ="aicc", approximation = FALSE)
#Auto Algorithm - Slow but more accurate
auto.arima(cheese.ts, trace= TRUE, ic ="aicc", approximation = FALSE, stepwise = FALSE)

```


#####b.4. Auto Algorithm - Slow but more accurate
```{r 3.b.4}

auto.arima(cheese.ts, trace= TRUE, ic ="aicc", approximation = FALSE, stepwise = FALSE)

```





####4.MODEL COMPARISON

#####a AIC
```{r 4.a, eval = FALSE}

AIC(arima(cheese.ts, order = c(1, 0, 0), seasonal = list(order = c(2,0,0), period = 12)),
    arima(cheese.ts, order = c(2, 0, 0), seasonal = list(order = c(2,0,0), period = 12)),
    arima(cheese.ts, order = c(0, 0, 3), seasonal = list(order = c(2,0,0), period = 12)))

```


#####.b BIC
```{r 4.b, eval = FALSE}

BIC(arima(cheese.ts, order = c(1, 0, 0), seasonal = list(order = c(2,0,0), period = 12)),
    arima(cheese.ts, order = c(2, 0, 0), seasonal = list(order = c(2,0,0), period = 12)),
    arima(cheese.tstsdata2, order = c(0, 0, 3), seasonal = list(order = c(2,0,0), period = 12)))

```


####.c. Residual Diagnostics


#####c.1. Residuals are Uncorrelated (White Noise)
Check whether the residuals look like white noise (Independent) p>0.05 then the residuals are independent (white noise)

```{r 4.c.1, eval = FALSE}

tsdisplay(residuals(cheese.ts))
Box.test(cheese.ts$residuals, lag = 20, type = "Ljung-Box")
# p-values shown for the Ljung-Box statistic plot are incorrect so calculate
#critical chi squared value
# Chi-squared 20 d.f. and critical value at the 0.05
qchisq(0.05, 20, lower.tail = F)
# Observed Chi-squared 13.584 < 31.41 so we don't reject null hypothesis
# It means residuals are independent or uncorrelated (white noise) at lags 1-20.

```


c.2. Residuals are normally distributed with mean zero whether the forecast errors are normally distributed

```{r 4.c.2, eval = FALSE}

qqnorm(finalmodel$residuals)
qqline(finalmodel$residuals) # Normality Plot

```


#####c3. Residuals have constant Variance
How to choose the num?ber of lags for the Ljung-Box test

For non-seasonal time series, 

Number of lags to test = minimum (10, length of time series / 5)
or simply take 10
For seasonal time series, 

Number of lags to test = minimum (2m, length of time series / 5)
where, m = period of seasonality
or 


####5.MODEL VALIDATION
####EVALUATION AND VALIDATING FORECASTING MODEL PERFORMANCE

Apply fitted model to later years of Fatalities data
fatal.fit.arima<- arima(fatal.data[1:41,2],order =c(0,1,0))
to obtain the 1-step ahead forecasts, we use the 
function forecast() from the forecast package

#####.a Forecast and plot of the next h periods
```{r 5.a, eval = FALSE}
library(forecast) 
ship.arima.forecast<-as.array(forecast(ship.fit.arima,h=29)) 
ship.arima.forecast 
par(mfrow=c(1,1),oma=c(0,0,0,0))
plot(ship.arima.forecast, ylab = "Automobile Shipments")

```


#####.b.1 Forecast accuracy measures on the log scale.
 in-sample one-step forecasts.
```{r 5.b.1, eval = FALSE}
fore<-accuracy(ship.fit.arima)

```


#####.b.2  out-of-sample one-step forecasts.
```{r 5.b.2, eval = FALSE}

shipfore<-accuracy(ship.arima.forecast)
shipfore
View(shipfore)
fore

```